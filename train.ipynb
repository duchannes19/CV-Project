{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Image processing\n",
    "import cv2\n",
    "from keras.api import *\n",
    "from keras.api.preprocessing import image\n",
    "from keras.api.preprocessing import image_dataset_from_directory \n",
    "\n",
    "# Model building\n",
    "import tensorflow as tf\n",
    "from keras.api.models import Sequential\n",
    "from keras.api.layers import Dense, Conv2D, MaxPooling2D, Dropout, Flatten, BatchNormalization\n",
    "from keras.api.optimizers import Adam\n",
    "from keras.api.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        print(\"Memory growth set for GPUs.\")\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n",
    "else:\n",
    "    print(\"No GPUs detected.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the CSV file\n",
    "data_path = \"dataset/fer2013/fer2013/fer2013.csv\"  # Update this path to your CSV file\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv(data_path)\n",
    "\n",
    "# Inspect the first few rows\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert pixel values from string to numpy array\n",
    "def preprocess_images(df):\n",
    "    images = []\n",
    "    for pixel_sequence in df[\"pixels\"]:\n",
    "        img_array = np.array(pixel_sequence.split(), dtype=\"float32\")\n",
    "        img_array = img_array.reshape(48, 48, 1)  # Reshape to 48x48 and single channel\n",
    "        img_array = img_array / 255.0  # Normalize pixel values to [0, 1]\n",
    "        images.append(img_array)\n",
    "    return np.array(images)\n",
    "\n",
    "# Preprocess images and extract labels\n",
    "X = preprocess_images(df)\n",
    "y = df[\"emotion\"].values  # Emotion labels\n",
    "\n",
    "# Check the shape of images and labels\n",
    "print(\"Image data shape:\", X.shape)  # Should be (num_samples, 48, 48, 1)\n",
    "print(\"Label data shape:\", y.shape)  # Should be (num_samples,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into training, validation, and test sets\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp)\n",
    "\n",
    "print(\"Training set shape:\", X_train.shape)\n",
    "print(\"Validation set shape:\", X_val.shape)\n",
    "print(\"Test set shape:\", X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define CNN model\n",
    "model = Sequential([\n",
    "    Conv2D(64, (3, 3), activation=\"relu\", input_shape=(48, 48, 1), padding=\"same\"),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    Dropout(0.1),\n",
    "    \n",
    "    Conv2D(128, (5, 5), activation=\"relu\", padding=\"same\"),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    Dropout(0.1),\n",
    "    \n",
    "    Conv2D(512, (3, 3), activation=\"relu\", padding=\"same\"),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    Dropout(0.1),\n",
    "    \n",
    "    Conv2D(512, (3, 3), activation=\"relu\", padding=\"same\"),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    Dropout(0.1),\n",
    "    \n",
    "    Flatten(),\n",
    "    Dense(256, activation=\"relu\"),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.1),\n",
    "    \n",
    "    Dense(512, activation=\"relu\"),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.1),\n",
    "    \n",
    "    Dense(7, activation=\"softmax\")\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=Adam(learning_rate=0.005),\n",
    "              loss=\"sparse_categorical_crossentropy\",\n",
    "              metrics=[\"accuracy\"])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define callbacks\n",
    "early_stopping = EarlyStopping(monitor=\"val_loss\", patience=5, restore_best_weights=True)\n",
    "model_checkpoint = ModelCheckpoint(\"model/emotions.keras\", save_best_only=True)\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_val, y_val),\n",
    "    epochs=30,\n",
    "    batch_size=32,\n",
    "    callbacks=[early_stopping, model_checkpoint]\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
